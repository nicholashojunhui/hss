{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/hss/blob/master/06_dozing_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAgz0mNvlBI8"
      },
      "source": [
        "# Video Activity Classifier Workshop\n",
        "\n",
        "In this workshop, we will be training a classifier using body positions extracted from video.\n",
        "\n",
        "![photo](https://github.com/lisaong/hss/blob/master/assets/istockphoto-476741742.jpg?raw=1)\n",
        "\n",
        "This follows up from [04_pose_estimation.ipynb](04_pose_estimation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRZkMQhO3ZjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a316db8-5dc3-4722-e2b6-67e131c6e425"
      },
      "source": [
        "# to run in colab, uncomment this\n",
        "!git clone https://github.com/nicholashojunhui/hss"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hss'...\n",
            "remote: Enumerating objects: 773, done.\u001b[K\n",
            "remote: Counting objects: 100% (367/367), done.\u001b[K\n",
            "remote: Compressing objects: 100% (304/304), done.\u001b[K\n",
            "remote: Total 773 (delta 79), reused 72 (delta 29), pack-reused 406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (773/773), 72.57 MiB | 15.60 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_PP-WW53nK4"
      },
      "source": [
        "# to run in colab, uncomment this\n",
        "HSS_DIR='/content/hss/PartC_OpenPose'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvFHRyvlBJA"
      },
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import paired_distances\n",
        "\n",
        "# requires: conda install opencv\n",
        "import cv2\n",
        "\n",
        "#plt.style.use('seaborn-white')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmVrGFvmlBJE"
      },
      "source": [
        "# Source: https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md#pose-output-format-coco\n",
        "Pose_part_pairs = [\n",
        "    (1,8), (1,2), (1,5), (2,3), (3,4), (5,6), (6,7), (8,9), (9,10), (10,11),\n",
        "    (8,12), (12,13), (13,14), (1,0), (0,15), (15,17), (0,16), (16,18), (2,17), (5,18),\n",
        "    (14,19), (19,20), (14,21), (11,22), (22,23), (11,24)\n",
        "]\n",
        "\n",
        "def draw_skeleton(ax, df):\n",
        "    \"\"\"Connects keypoints into a skeleton\"\"\"\n",
        "    for p, q in Pose_part_pairs:\n",
        "        if df.x[p] != 0 and df.x[q] != 0 and df.y[p] != 0 and df.y[q] != 0:\n",
        "            ax.plot([df.x[p], df.x[q]], [df.y[p], df.y[q]], color='red')\n",
        "\n",
        "def keypoints_to_dataframe(keypoints):\n",
        "    \"\"\"Converts a flat keypoints list (x1, y1, c1, x2, y2, c2) into a pandas DataFrame\"\"\"\n",
        "    return pd.DataFrame({'x': keypoints[::3], 'y': keypoints[1::3], 'c': keypoints[2::3]})\n",
        "\n",
        "def get_centroid(coordinates, threshold=0.1):\n",
        "    \"\"\"Computes the centroid of a given 2 dimensional vector\"\"\"\n",
        "    x = coordinates[coordinates.c > threshold].x\n",
        "    y = coordinates[coordinates.c > threshold].y\n",
        "\n",
        "    return [sum(x)/len(x), sum(y)/len(y)]\n",
        "\n",
        "def get_centroids(frame):\n",
        "    \"\"\"Returns the centroid for each person as a list of (x, y) coordinates\"\"\"\n",
        "    return np.array([get_centroid(keypoints_to_dataframe(person['pose_keypoints_2d'])) for person in frame['people']])\n",
        "\n",
        "def get_closest_index(centroid, other_frame):\n",
        "    \"\"\"Find closest index in other_frame from a given centroid\"\"\"\n",
        "    other_centroids = get_centroids(other_frame)\n",
        "    return np.argmin(paired_distances(np.ones(other_centroids.shape) * centroid, other_centroids))\n",
        "\n",
        "def plot_keypoints(video_path, keypoints_path, frame_first, frame_step, max_frame_last):\n",
        "    \"\"\"Displays the keypoints overlaid on the video\"\"\"\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # fast forward video to frame_first\n",
        "    for i in range(frame_first):\n",
        "        _, image = video.read()\n",
        "\n",
        "    frame = json.load(open(f'{keypoints_path}_{frame_first:012d}_keypoints.json', 'rb'))\n",
        "    df = keypoints_to_dataframe(frame['people'][0]['pose_keypoints_2d'])\n",
        "    centroid = get_centroid(df)\n",
        "\n",
        "    # Create the matplotlib axes\n",
        "    fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), sharex=True, sharey=True)\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, t in zip(range(len(ax)), range(frame_first, max_frame_last, frame_step)):\n",
        "        _, image = video.read()\n",
        "\n",
        "        frame = json.load(open(f'{keypoints_path}_{t:012d}_keypoints.json', 'rb'))\n",
        "        index = get_closest_index(centroid, frame) # find the closest person\n",
        "\n",
        "        # load keypoints for the closest person\n",
        "        df = keypoints_to_dataframe(frame['people'][index]['pose_keypoints_2d'])\n",
        "\n",
        "        axis = ax[i]\n",
        "        axis.scatter(df.x, df.y, s=df.c*10, color='yellow')\n",
        "        axis.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        axis.set(title=f'Frame {t}')\n",
        "        draw_skeleton(axis, df)\n",
        "\n",
        "        centroid = get_centroid(df) # update centroid since person may have moved"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oEC9Dz7lBJM"
      },
      "source": [
        "## Examine the keypoints\n",
        "\n",
        "Plot a sample from each class, with 6 frames and skeleton overlay.\n",
        "\n",
        "**Important**: make sure `HSS_DIR` is set correctly so that the subsequent code can find the keypoint JSON files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVjds0lgHazo"
      },
      "source": [
        "# Use this path if running on Windows\n",
        "#HSS_DIR = r'D:\\S-HSS\\Workshop\\hss'\n",
        "\n",
        "# Use this path if running on Colab\n",
        "HSS_DIR = r'/content/hss/PartC_OpenPose'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRQPetFelBJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "73674194-f869-412c-b002-41f54172ae66"
      },
      "source": [
        "def plot_sample(classname, sample_index):\n",
        "    \"\"\"Plots first few frames of a sample video\"\"\"\n",
        "    data_path = os.path.join(HSS_DIR, 'data', 'speed', classname)\n",
        "    video_path = os.path.join(data_path, f'{sample_index}.mp4')\n",
        "    keypoints_path = os.path.join(data_path, str(sample_index), str(sample_index))\n",
        "\n",
        "    frame_first = 20\n",
        "    frame_step = 1\n",
        "    frame_last = 50\n",
        "\n",
        "    plot_keypoints(video_path, keypoints_path, frame_first, frame_step,\n",
        "                   max_frame_last=frame_last)\n",
        "\n",
        "plot_sample('run', 1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hss/PartC_OpenPose/data/speed/run/1/1_000000000020_keypoints.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b009751b8c30>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                    max_frame_last=frame_last)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplot_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b009751b8c30>\u001b[0m in \u001b[0;36mplot_sample\u001b[0;34m(classname, sample_index)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mframe_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     plot_keypoints(video_path, keypoints_path, frame_first, frame_step,\n\u001b[0m\u001b[1;32m     12\u001b[0m                    max_frame_last=frame_last)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-75c173cfdaf5>\u001b[0m in \u001b[0;36mplot_keypoints\u001b[0;34m(video_path, keypoints_path, frame_first, frame_step, max_frame_last)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{keypoints_path}_{frame_first:012d}_keypoints.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeypoints_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'people'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pose_keypoints_2d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcentroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_centroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hss/PartC_OpenPose/data/speed/run/1/1_000000000020_keypoints.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipaf9O7TlBJR"
      },
      "source": [
        "plot_sample('walk', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyVhLEIOzNAg"
      },
      "source": [
        "## Extract Features\n",
        "\n",
        "This part and the part below can be run from any environment.\n",
        "\n",
        "The next step is to convert the keypoints into features for training a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhd_eYiklBJU"
      },
      "source": [
        "def get_part_candidates_as_features(keypoints_path, frame_first, frame_step,\n",
        "                                    max_frame_last):\n",
        "    \"\"\"Convert keypoints into features for body parts\n",
        "\n",
        "    BODY_25 format:\n",
        "    https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md#keypoint-ordering-in-cpython\n",
        "        //     {0,  \"Nose\"},\n",
        "        //     {1,  \"Neck\"},\n",
        "        //     {2,  \"RShoulder\"},\n",
        "        //     {3,  \"RElbow\"},\n",
        "        //     {4,  \"RWrist\"},\n",
        "        //     {5,  \"LShoulder\"},\n",
        "        //     {6,  \"LElbow\"},\n",
        "        //     {7,  \"LWrist\"},\n",
        "        //     {8,  \"MidHip\"},\n",
        "        //     {9,  \"RHip\"},\n",
        "        //     {10, \"RKnee\"},\n",
        "        //     {11, \"RAnkle\"},\n",
        "        //     {12, \"LHip\"},\n",
        "        //     {13, \"LKnee\"},\n",
        "        //     {14, \"LAnkle\"},\n",
        "        //     {15, \"REye\"},\n",
        "        //     {16, \"LEye\"},\n",
        "        //     {17, \"REar\"},\n",
        "        //     {18, \"LEar\"},\n",
        "        //     {19, \"LBigToe\"},\n",
        "        //     {20, \"LSmallToe\"},\n",
        "        //     {21, \"LHeel\"},\n",
        "        //     {22, \"RBigToe\"},\n",
        "        //     {23, \"RSmallToe\"},\n",
        "        //     {24, \"RHeel\"},\n",
        "        //     {25, \"Background\"}\n",
        "\n",
        "    Result:\n",
        "        pandas Dataframe: 3 columns for each part, 1 row per frame\n",
        "        Nose_x, Nose_y, Nose_conf, Neck_x, Neck_y, Neck_confidence, ..\n",
        "    \"\"\"\n",
        "    PARTS = [\"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\",\n",
        "             \"LShoulder\", \"LElbow\", \"LWrist\", \"MidHip\", \"RHip\",\n",
        "             \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"REye\",\n",
        "             \"LEye\", \"REar\", \"LEar\", \"LBigToe\", \"LSmallToe\",\n",
        "             \"LHeel\", \"RBigToe\", \"RSmallToe\", \"RHeel\", \"Background\"]\n",
        "    PARTS_INDEX = {PARTS[i]: i for i in range(len(PARTS))}\n",
        "\n",
        "    selected_parts = [\"RWrist\", \"RShoulder\", \"RElbow\", \"RWrist\",\n",
        "             \"LShoulder\", \"LElbow\", \"LWrist\", \"MidHip\", \"RHip\",\n",
        "             \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"LBigToe\",\n",
        "             \"LSmallToe\", \"LHeel\", \"RBigToe\", \"RSmallToe\", \"RHeel\"]\n",
        "\n",
        "    selected_indices = [PARTS_INDEX[p] for p in selected_parts]\n",
        "\n",
        "    colnames = []\n",
        "    for p in selected_parts:\n",
        "        colnames += [f'{p}_x', f'{p}_y', f'{p}_c']\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for t in range(frame_first, max_frame_last, frame_step):\n",
        "        frame = json.load(open(f'{keypoints_path}_{t:012d}_keypoints.json', 'rb'))\n",
        "        part_candidates = frame['part_candidates'][0]\n",
        "\n",
        "        row = []\n",
        "        # list comprehension can't be used as some part may be missing\n",
        "        for i in selected_indices:\n",
        "            part = part_candidates[str(i)]\n",
        "            if len(part) == 0:\n",
        "                # part not found\n",
        "                part = [0, 0, 0]\n",
        "            row += part[:3] # some parts appear twice, just pick the first entry\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows, columns=colnames)\n",
        "\n",
        "def get_features(classname, sample_index):\n",
        "    data_path = os.path.join(HSS_DIR, 'data', 'speed', classname)\n",
        "    video_path = os.path.join(data_path, f'{sample_index}.mp4')\n",
        "    keypoints_path = os.path.join(data_path, str(sample_index), str(sample_index))\n",
        "\n",
        "    frame_first = 20\n",
        "    frame_step = 1\n",
        "    frame_last = 30\n",
        "\n",
        "    return get_part_candidates_as_features(keypoints_path, frame_first,\n",
        "                                    frame_step, max_frame_last=frame_last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ2oIRN8nfZU"
      },
      "source": [
        "df_run = get_features('run', 1)\n",
        "df_run.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK_v-Vu0qS0F"
      },
      "source": [
        "df_walk = get_features('walk', 1)\n",
        "df_walk.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSLJkBu8y_Yw"
      },
      "source": [
        "##<font color='red'>Exercise</font>\n",
        "\n",
        "Currently the body parts selected are \"Nose\", \"Neck\", \"Right Wrist\", and \"Left Wrist\".\n",
        "\n",
        "1. Consider whether other body parts are more suitable for distinguishing between the two classes (sleeping or awake).\n",
        "\n",
        "2. Under `get_part_candidates_as_features` function above, update `selected_parts` to use those body parts. Once you are done, re-run the above cells and proceed to the next section to create the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0CGMSixJ2x"
      },
      "source": [
        "## Create our dataset\n",
        "\n",
        "* X - features\n",
        "* y - target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-1kxIf-xRIK"
      },
      "source": [
        "num_samples = 3\n",
        "\n",
        "# row-wise concat\n",
        "df_run = pd.concat([get_features('run', i) for i in range(1, num_samples+1)])\n",
        "df_walk = pd.concat([get_features('walk', i) for i in range(1, num_samples+1)])\n",
        "\n",
        "X = pd.concat([df_run, df_walk])\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vKOw2lx4WI"
      },
      "source": [
        "classes = ['run', 'walk']\n",
        "y = np.array([0] * df_run.shape[0] + [1] * df_walk.shape[0])\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZi7rBuHyjlU"
      },
      "source": [
        "## Visualise our training set\n",
        "\n",
        "We will use a technique called PCA to reduce the features into 2 dimensions, then plot the two classes.\n",
        "\n",
        "This is useful to indicate whether the model used is going perform terribly. If there is overlap for the samples, then a model cannot tell between them. If there is spacing and clear separation between the samples (social distancing!), then the model can distinguish them by drawing boundary curves, lines, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpOE78Ez_OX"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_2d = pca.fit_transform(scaler.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W5iOu5Z0kaV"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_title('2-d visualization of data')\n",
        "ax.scatter(X_2d[y==0, 0], X_2d[y==0, 1], label=classes[0])\n",
        "ax.scatter(X_2d[y==1, 0], X_2d[y==1, 1], label=classes[1])\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcIEW94o0-U7"
      },
      "source": [
        "# analogous to the degree of 2-d \"compression\". A high number (close to 1)\n",
        "# indicates that the 2-d PCA is non-lossy (so we can trust the above diagram more).\n",
        "# above 50% is quite \"okay\", meaning that about 50% loss. Typically, for these\n",
        "# dimensionality reduction methods, it can be as low as 20-30%.\n",
        "\n",
        "pca.explained_variance_ratio_.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF3fpLzA1QBL"
      },
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "Looks like a Random Forest should be able to distinguish between the orange and blue samples, because there are some boundaries that can be drawn to separate them. You can imagine a classifier as a program that automatically tries to draw some line between the clusters.\n",
        "\n",
        "Let's give it a try.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EFf0OzT1WY8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train-test split into test and training sets\n",
        "# stratify: preserves the proportions of the classes\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=25)\n",
        "rf.fit(X_train, y_train)\n",
        "rf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDR13HrM2znT"
      },
      "source": [
        "# Get predictions from the classifier\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# classification report:\n",
        "#   support: number of samples in each class\n",
        "#   accuracy - how many are correct\n",
        "#   precision - how well the classifier avoided false positives\n",
        "#   recall - how well the classifier avoided false negatives\n",
        "#   weighted avg - average weighted based on the proportion of samples in each class\n",
        "#   macro avg - global average\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5756RFg_4vHZ"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9wN3FSU4qVL"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isUfFaPH4tW0"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybJxGpei3Sm0"
      },
      "source": [
        "## <font color='red'> Exercise </font>\n",
        "\n",
        "1. Instead of asleep vs. awake, replace the videos with two other types of activity.\n",
        "\n",
        "2. Train a classifier to predict between your chosen activities.\n",
        "\n",
        "### <font color='red'> Submission </font>\n",
        "Submit your modified .ipynb to the Workshop 4 folder by the submission deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2dDLA694au9"
      },
      "source": [
        "## Appendix: Hidden Markov Models\n",
        "\n",
        "*This section is for your reference*\n",
        "\n",
        "Instead of a Random Forest Classifier, which is a machine learning model that tries to look at the spatial features of each frame (row) independently to predict the classes, we can try a model that looks at the time-sequence, treating each row as a step in time.\n",
        "\n",
        "HMMs are statistical models that can learn sequences. Alternatively, an RNN (Recurrent Neural Network) is a deep learning model that learns sequences.\n",
        "\n",
        "To use HMMs, we need to preserve the sequence ordering in the rows. Previously, when we did `train_test_split`, the rows are shuffled so ordering is lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ_CgUn85eGn"
      },
      "source": [
        "!pip install hmmlearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94tPNoPT4abL"
      },
      "source": [
        "# Rows in their original frame (row) order\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9IDHJwh5Oc9"
      },
      "source": [
        "# (rows, features)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8hbZTe67PNz"
      },
      "source": [
        "# split into separate classes to build separate HMMs, one per class\n",
        "\n",
        "X_class0 = X[y==0]\n",
        "X_class1 = X[y==1]\n",
        "\n",
        "X_class0.shape, X_class1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXZE3ZQU71gr"
      },
      "source": [
        "# split into train and test without shuffling\n",
        "X_class0_train, X_class0_test = train_test_split(X_class0,\n",
        "                                                 shuffle=False,\n",
        "                                                 test_size=.1)\n",
        "X_class0_train.shape, X_class0_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc3_aAZn9Bzv"
      },
      "source": [
        "# split into train and test without shuffling\n",
        "X_class1_train, X_class1_test = train_test_split(X_class1,\n",
        "                                                 shuffle=False,\n",
        "                                                 test_size=.1)\n",
        "X_class1_train.shape, X_class1_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqGeyDI__OUC"
      },
      "source": [
        "# Train our hmms\n",
        "import hmmlearn.hmm as hmm\n",
        "\n",
        "def fit_hmm(X, window_size):\n",
        "  \"\"\"Fits a Gaussian HMM using training set\n",
        "  X: training set in (rows, features)\n",
        "  window_size: length of each sequence to consider, number of rows\n",
        "               should be divisible by it\n",
        "  \"\"\"\n",
        "  rows = X.shape[0]\n",
        "  lengths = [window_size] * (rows//window_size) # integer division\n",
        "  return hmm.GaussianHMM(n_components=window_size,\n",
        "                         algorithm='viterbi',\n",
        "                         random_state=42,\n",
        "                         verbose=True).fit(X, lengths)\n",
        "\n",
        "window_size = 2\n",
        "training_set = [X_class0_train, X_class1_train]\n",
        "hmms = [fit_hmm(X_t, window_size) for X_t in training_set]\n",
        "hmms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_o-LswAeTG"
      },
      "source": [
        "def predict(hmms, X_test, window_size):\n",
        "  \"\"\"Predict the class that the test set belongs to\n",
        "  hmms: list of already fitted HMMs\n",
        "  X_test: test set in (rows, features)\n",
        "  window_size: length of each sequence to consider. Should be\n",
        "               same as what was used during fitting\n",
        "  \"\"\"\n",
        "  lengths = [window_size] * (X_test.shape[0]//window_size)\n",
        "  scores = np.array([hmm.score(X_test, lengths) for hmm in hmms])\n",
        "  print(scores)\n",
        "  print(f'Prediction: class {scores.argmax()}, log probability: {scores.max()}')\n",
        "\n",
        "#\n",
        "# Check if the correct class is predicted.\n",
        "# To tune HMMs, you would play around with the window_size, the algorithm\n",
        "# ('map' or 'viterbi'), and so on\n",
        "# https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GaussianHMM\n",
        "#\n",
        "predict(hmms, X_class0_test, window_size=window_size)\n",
        "predict(hmms, X_class1_test, window_size=window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJxquK5IEb9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}